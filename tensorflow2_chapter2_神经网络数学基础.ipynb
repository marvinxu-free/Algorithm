{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow2_chapter2_神经网络数学基础.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLLJ364AZ/TIH9uoQryen4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvinxu-free/Algorithm/blob/master/tensorflow2_chapter2_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMepSCKP_Ckn",
        "colab_type": "text"
      },
      "source": [
        "# 神经网络数学基础\n",
        "\n",
        "主要为了加深神经网络基础数据的了解\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq7rAIBI_9S5",
        "colab_type": "text"
      },
      "source": [
        "## 初识神经网络\n",
        "神经网络的基本组件是layer，可以把它当成数据过滤器：\n",
        "\n",
        "- 进入的是一些数据，出来的数据是更加有效的数据\n",
        "\n",
        "要想训练网络，还需要编译三个关键参数：\n",
        "\n",
        "1. 损失函数（loss function): 如何衡量网络在训练数据上的性能。\n",
        "2. 优化器（optimizer）：基于训练数据和损失函数来更新网络。\n",
        "3. 在训练和测试过程中需要监控的指标（metric): 比如准确率，auc，recall等。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzRMQAHI_BQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "train_x, test_x = train_x/255.0, test_x/255.0\n",
        "epoch = 10"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kTpSewgCZNM",
        "colab_type": "text"
      },
      "source": [
        "## keras function API\n",
        "\n",
        "keras定义模型有两种方式：\n",
        "- 使用Sequential类（仅用于基于层的线性堆叠，这是目前最常用的网络架构），但是结构比较简单，代码可读性较差\n",
        "- function API模式，用于层组成的有向无环图，可以构建任意形式的架构.\n",
        "- function API也有缺点：不支持动态架构，这在大部分的神经网络中都是不需要的，但是对于递归的神经网络或者tree RNN中必须要有动态架构。\n",
        "\n",
        "因此，一般如果网络不需要支持recurse编程的话，使用函数式编程会比较好，如果需要支持的话，那么就需要混合使用各种编程模式。\n",
        "\n",
        "比如简单的mnist数据集上的分类模型："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFjWTP2NX7Gc",
        "colab_type": "text"
      },
      "source": [
        "### subclass模式（一般不建议使用）\n",
        "\n",
        "subclass模式值得是模型代码通过形式`tf.keras.Model`的构建模型，使用这种方式构建模型，有个特点，就是在模型类中必须手工重载实现其 call() 方法。换句话来说，就是 call() 方法的参数 training 必须由你来管理了。怎么理解呢，下面举个例子：\n",
        "\n",
        "```python\n",
        "def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()   \n",
        "        self.dense1 = tf.keras.layers.Dense(units=100)\n",
        "        self.bn = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        "\n",
        "def call(self, inputs, training=None):         \n",
        "    x = self.flatten(inputs)    \n",
        "    x = tf.nn.relu(self.bn(self.dense1(x), training=training))  \n",
        "    output = tf.nn.softmax(self.dense2(x))\n",
        "    return output\n",
        "```\n",
        "\n",
        "在上面这个例子中，BatchNormalization 层的实例 self.bn 在调用过程中，需要我们手工传入 training 参数，使得它在训练模式和推理模式下表现出不同的行为。\n",
        "\n",
        "而在前面两种构建模型的方式中，我们不需要手工管理 training 参数，只需要在使用模型类实例 model 时，隐式调用其 call() 方法，传入正确的 training 参数值即可，TensorFlow 在底层构建模型时就会根据我们传入的 training 参数值，自动给 BatchNormalization 层等赋予不同的行为。\n",
        "\n",
        "所以，在前面两种构建模型的方式中，我们不需要在 BatchNormalization 层的实例被调用时传入 training 参数值，而第三种方式则需要！\n",
        "\n",
        "此外subclass模式出来的模型不能直接调用summary()函数查看网络架构。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtTBpytgByar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_class=10):\n",
        "    super(MyModel, self).__init__()\n",
        "    # 定义layer\n",
        "    # inputs = tf.keras.Input(shape=(28,28)) # 返回一个placeholder用于数据输入\n",
        "    self.x0 = tf.keras.layers.Flatten()\n",
        "    self.x1 = tf.keras.layers.Dense(512, activation='relu', name='d1')\n",
        "    self.x2 = tf.keras.layers.Dropout(0.2)\n",
        "    self.predictions = tf.keras.layers.Dense(10, activation='softmax', name='d2')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    function API定义前向传递计算逻辑的范式，\n",
        "    基于的是Python的科利华编程技巧\n",
        "\n",
        "    \"\"\"\n",
        "    # 使用前面`__init__`定义的神经网络层\n",
        "    x = self.x0(inputs)\n",
        "    x = self.x1(x)\n",
        "    x = self.x2(x)\n",
        "    return self.predictions(x)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtT-UqWtV1sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MyModel()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPWj3Ye-E98V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "16d50681-87e3-44a3-c800-2f5f2eb235e7"
      },
      "source": [
        "optimiser = tf.keras.optimizers.Adam()\n",
        "model.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(train_x, train_y, batch_size=32, epochs=epoch)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2185 - accuracy: 0.9347\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0964 - accuracy: 0.9704\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0694 - accuracy: 0.9786\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0529 - accuracy: 0.9829\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0431 - accuracy: 0.9858\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0368 - accuracy: 0.9876\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0320 - accuracy: 0.9894\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0269 - accuracy: 0.9907\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0248 - accuracy: 0.9918\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0229 - accuracy: 0.9923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07d34a3d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEWU10hMYzkj",
        "colab_type": "text"
      },
      "source": [
        "### function API\n",
        "\n",
        "比较pure的function API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw-66D2sHNkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use keras functional API\n",
        "inputs = tf.keras.Input(shape=(28,28))  # Returns a placeholder tensor\n",
        "x = tf.keras.layers.Flatten()(inputs)\n",
        "x = tf.keras.layers.Dense(512, activation='relu',name='d1')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "predictions = tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='d2')(x)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r9dJ_QxWG4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3 = tf.keras.Model(inputs=inputs, outputs=predictions)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tyF9YnEWJfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "a7859936-5de8-4ea4-9853-c82326d3933b"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 28, 28)]          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "d1 (Dense)                   (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "d2 (Dense)                   (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wod3QqIWKr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimiser = tf.keras.optimizers.Adam()\n",
        "model3.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNYMGiksWM-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "eea48ae1-a177-445f-b6cb-4f403a23966b"
      },
      "source": [
        "model3.fit(train_x, train_y, batch_size=32, epochs=epoch)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2208 - accuracy: 0.9342\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0956 - accuracy: 0.9699\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0688 - accuracy: 0.9781\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0519 - accuracy: 0.9839\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0421 - accuracy: 0.9864\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0359 - accuracy: 0.9882\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0316 - accuracy: 0.9896\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0272 - accuracy: 0.9911\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0242 - accuracy: 0.9924\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0228 - accuracy: 0.9927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07d629bcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HReepUadZFtW",
        "colab_type": "text"
      },
      "source": [
        "## 神经网络数学表示\n",
        "\n",
        "### 张量（tensor)\n",
        "张量是一个数据容器，其包含的数据几乎都是数值数据，因此其是一个数字容器。比如矩阵，其就是一个二维张量。\n",
        "\n",
        "张量是矩阵向任意维度的推广，==**注意张量的维度通常叫做axis**==\n",
        "\n",
        "### 标量（0D张量）\n",
        "\n",
        "仅包含一个数字的张量，叫做标量，也叫作零维张量、0D张量。在numpy中，一个数字就是一个标量，可以通过np.ndim查看一个标量的轴的个数。比如:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kim0b8uQWOsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23f20eb7-72bd-418e-ce29-c9cb72181a02"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(1)\n",
        "x.ndim"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9b6uconav4n",
        "colab_type": "text"
      },
      "source": [
        "### 向量（1D张量）\n",
        "\n",
        "数字组成的数组叫做向量(vector)或者一维张量（1D张量），一维张量只有一个轴。比如:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g4B6KRoapxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4a203ee-2cae-4c47-ebd2-054dcf6cac37"
      },
      "source": [
        "a = np.arange(10)\n",
        "a.ndim"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Ca2t7Dd_DR",
        "colab_type": "text"
      },
      "source": [
        "### dimension容易混淆的点\n",
        "上面的a有10个元素，在很多概念里面dimension既能表示沿某个轴的元素的个数（比如5D向量),也可以表示向量中轴的个数（比如5D张量），有时候会让人感到混乱。\n",
        "对于后一种情况，严格的说应该是5阶张量（张量的阶数代表的就是轴的个数），但是5D张量的说法更加常见。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRtqjLoKfagC",
        "colab_type": "text"
      },
      "source": [
        "### 矩阵（2D张量）\n",
        "向量组成的数组称作矩阵（matrix）或者2D张量。矩阵有两个轴，通常第一和称作行，第二个称作列。。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk7rEZp5bNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd03fd9e-ac54-40c1-9483-af00ddcfbfef"
      },
      "source": [
        "x = np.arange(100).reshape([10,10])\n",
        "x.ndim"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-ciFO2gLlS",
        "colab_type": "text"
      },
      "source": [
        "### 3D张量或者更高维度的张量\n",
        "将多个矩阵组合成数组，就变成3D张量。\n",
        "将多个3D张量组合成数组，就变成了4D张量。\n",
        "将多个4D张量组合成数组，就变成了5D张量。\n",
        "\n",
        "深度学习处理的数据一般是0-4D的张量，但处理视频数据的时候回用到5D张量."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8RTvr_KgrzN",
        "colab_type": "text"
      },
      "source": [
        "### 张量的关键属性\n",
        "张量有以下几个关键属性:\n",
        "1. 轴的个数（阶）：例如3D张量有3个轴，4D张量有4个轴。\n",
        "2. 形状：这是一个整数元祖，代表每一个轴上面的维度大小(元素的个数)，例如前面的矩阵的形状就是(10,10).\n",
        "3. 数据类型：张量中所包含的数据类型，一般不包括字符串类型，因为张量存储的是预先分配的连续内存段，而字符串是变长的，无法用这种方式存储。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3jR8kWjhwoB",
        "colab_type": "text"
      },
      "source": [
        "### 张量切片操作\n",
        "选择张量的特定元素叫做张量的切片操作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNylICgOgFh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "88938cfa-8036-4f2b-dcff-b883c220bf7b"
      },
      "source": [
        "x = np.arange(100).reshape([10,10])\n",
        "print(x.shape)\n",
        "x[1:10, 7].shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VakMtGniVsz",
        "colab_type": "text"
      },
      "source": [
        "### 数据批量的概念\n",
        "通常来说，在深度学习中第一个轴都是样本轴（sample axis，0轴，因为索引是从0开始）。在MNIST的例子中，样本就是图像。\n",
        "\n",
        "此外，在深度学习中，模型不会同时处理整个数据集，而是将数据分为小批量，具体就是MNIST数据集的一个批量，批量大小为128.\n",
        "`batch = train_image[:128]`\n",
        "然后是下一个批量:\n",
        "`batch = train_image[128:256]`\n",
        "\n",
        "**对于这种批量张量，第一个轴（0轴）叫做批量轴**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7-eak0Bjozn",
        "colab_type": "text"
      },
      "source": [
        "### 现实中的数据张量\n",
        "\n",
        "遇到的数据示例几乎是以下之一：\n",
        "\n",
        "- 向量数据： 2D张量，形状为(samples, features)：比如人口统计数据，每个人表示为包含年龄、学历的向量，10000个人就是(10000,3)的2D张量。\n",
        "- 时间序列或者序列数据: 3D张量(samples, timestamp, features)\n",
        "- 图像: 4D张量(samples, height, width, channel)\n",
        "- 视频: 5D张量(samples,frame, height, width, channel)或者(samples,frame, channel, height, width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jRheiAdoJut",
        "colab_type": "text"
      },
      "source": [
        "### 张量运算\n",
        "神经网络学到额所有变化，都可以简化为数值数据张量上的一些张量运算(tensor operation)。例如加上张量、乘以张量。\n",
        "\n",
        "\n",
        "#### 逐元素运算（element-wise)\n",
        "\n",
        "element-wise运算，即张量中的所有元素可以独立计算，非常适合大规模的并行计算。\n",
        "\n",
        "比如矩阵加法: `x + x`\n",
        "\n",
        "逐元素运算，只支持两个形状相同的张量。\n",
        "\n",
        "#### 广播运算\n",
        "如果两个形状不同的张量相加,在没有歧义的情况下将小的张量进行广播，广播包含以下两个步骤：\n",
        "- 向较小的张量添加轴， 是ndim与大的相同\n",
        "- 将较小的张量沿着新轴复制，使得两者大小相同。\n",
        "- 然后进行逐元素计算。\n",
        "\n",
        "实际中并不会真的复制，只是在算法里面计算逻辑有体现。\n",
        "\n",
        "#### 点积运算\n",
        "点积运算，也叫作张量积（Tensor product， 不要和element-wise计算混淆),**是最有用的也是最重要的张量运算**。与逐元素计算不同，它将输入的元素合并到一起。\n",
        "\n",
        "有以下几个点：\n",
        "- 向量的点击是标量。\n",
        "- 如果一个张量的ndim大于1，点积不满足交换律。\n",
        "- 对于矩阵x和y， 当且仅当x.shape[1]==y.shape[0]的时候，才能进行点积dot(x,y)\n",
        "- 更一般的说，对更高的维度进行点积，其形状必须遵循前面的2D向量相同的原则.\n",
        "\n",
        "```python\n",
        "(a,b,c,d) . (d) -> (a, b, c)\n",
        "(a,b,c,d) . (d,e) -> (a,b,c,e)\n",
        "```\n",
        "#### 张量变形\n",
        "要求size相等。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulglQPX-sRka",
        "colab_type": "text"
      },
      "source": [
        "### 张量运算的几何解释\n",
        "可以将神经网络解释为高维空间非常复杂的几何变换。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGi4aPzXiBwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}